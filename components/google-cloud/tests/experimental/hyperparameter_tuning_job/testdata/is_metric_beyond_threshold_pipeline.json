{
  "components": {
    "comp-is-metric-beyond-threshold": {
      "executorLabel": "exec-is-metric-beyond-threshold",
      "inputDefinitions": {
        "parameters": {
          "study_spec_metrics": { "parameterType": "LIST" },
          "threshold": { "parameterType": "NUMBER_DOUBLE" },
          "trial": { "parameterType": "STRING" }
        }
      },
      "outputDefinitions": {
        "parameters": { "output": { "parameterType": "STRING" } }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-is-metric-beyond-threshold": {
        "container": {
          "args": [
            "--trial",
            "{{$.inputs.parameters['trial']}}",
            "--study-spec-metrics",
            "{{$.inputs.parameters['study_spec_metrics']}}",
            "--threshold",
            "{{$.inputs.parameters['threshold']}}",
            "----output-paths",
            "{{$.outputs.parameters['output'].output_file}}"
          ],
          "command": [
            "sh",
            "-c",
            "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.18.3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.18.3' --user) && \"$0\" \"$@\"",
            "sh",
            "-ec",
            "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
            "def is_metric_beyond_threshold(trial, study_spec_metrics,\n                              threshold):\n  \"\"\"Determines if the metric of the best trial beyond the threshold given.\n\n  Args:\n      trial (str): Required. The intermediate JSON representation of a\n        hyperparameter tuning job trial.\n      study_spec_metrics (list): Required. List serialized from dictionary\n        representing the metrics to optimize.\n        The dictionary key is the metric_id, which is reported by your training\n        job, and the dictionary value is the optimization goal of the metric\n        ('minimize' or 'maximize'). example:\n        metrics = hyperparameter_tuning_job.serialize_metrics(\n            {'loss': 'minimize', 'accuracy': 'maximize'})\n      threshold (float): Required. Threshold to compare metric against.\n\n  Returns:\n      \"true\" if metric is beyond the threshold, otherwise \"false\"\n\n  Raises:\n      RuntimeError: If there are multiple metrics.\n  \"\"\"\n  from google.cloud.aiplatform_v1.types import study\n\n  if len(study_spec_metrics) > 1:\n    raise RuntimeError('Unable to determine best parameters for multi-objective'\n                       ' hyperparameter tuning.')\n  trial_proto = study.Trial.from_json(trial)\n  val = trial_proto.final_measurement.metrics[0].value\n  goal = study_spec_metrics[0]['goal']\n\n  is_beyond_threshold = False\n  if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n    is_beyond_threshold = val > threshold\n  elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n    is_beyond_threshold = val < threshold\n\n  return 'true' if is_beyond_threshold else 'false'\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n    return str_value\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Is metric beyond threshold', description='Determines if the metric of the best trial beyond the threshold given.')\n_parser.add_argument(\"--trial\", dest=\"trial\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--study-spec-metrics\", dest=\"study_spec_metrics\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--threshold\", dest=\"threshold\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = is_metric_beyond_threshold(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          ],
          "image": "python:3.10"
        }
      }
    }
  },
  "pipelineInfo": { "name": "is-metric-beyond-threshold-op-test" },
  "root": {
    "dag": {
      "tasks": {
        "is-metric-beyond-threshold": {
          "cachingOptions": { "enableCache": true },
          "componentRef": { "name": "comp-is-metric-beyond-threshold" },
          "inputs": {
            "parameters": {
              "study_spec_metrics": {
                "runtimeValue": {
                  "constant": [{ "goal": 1.0, "metric_id": "accuracy" }]
                }
              },
              "threshold": { "runtimeValue": { "constant": 0.5 } },
              "trial": {
                "runtimeValue": {
                  "constant": "{\n \"id\": \"1\",\n \"state\": 4,\n \"parameters\": [\n {\n \"parameterId\": \"learning_rate\",\n \"value\": 0.03162277660168379\n },\n {\n \"parameterId\": \"momentum\",\n \"value\": 0.5\n },\n {\n \"parameterId\": \"num_neurons\",\n \"value\": 128.0\n }\n ],\n \"finalMeasurement\": {\n \"stepCount\": \"10\",\n \"metrics\": [\n {\n \"metricId\": \"accuracy\",\n \"value\": 0.734375\n }\n ]\n },\n \"startTime\": \"2021-12-10T00:41:57.675086142Z\",\n \"endTime\": \"2021-12-10T00:52:35Z\",\n \"name\": \"\",\n \"measurements\": [],\n \"clientId\": \"\",\n \"infeasibleReason\": \"\",\n \"customJob\": \"\"\n}"
                }
              }
            }
          },
          "taskInfo": { "name": "is-metric-beyond-threshold" }
        }
      }
    }
  }
}
